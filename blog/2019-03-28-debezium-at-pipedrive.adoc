= Debezium at Pipedrive
ekoplimets
:awestruct-tags: [ examples, microservices, pipedrive, apache-kafka ]
:awestruct-layout: blog-post

[role="teaser"]
--
Pipedrive started looking into change data capture solutions for MySQL in the beginning of 2018 and now have most of the production MySQL databases producing real-time CDC events using Debezium. The following blog post describes our setup, various problems we have encountered and how we resolved them.
--

To start of with some numbers to get an idea of scale - we have in production environment *250+ MySQL active-passive server pairs* and one huge *10 node MySQL multi-master xtraDB cluster* spanning multiple data centers. Average incoming messages rate to kafka clusters is around 1500 msg/sec with peaks up to 10k msg/sec.

Our initial goals were to have real-time and 100% consistent data source for services depending on our main data entities, like full text search, statistics database and data warehouse. It has also helped some of our services to remove a hard dependency on message queues availability (the dual write problem mentioned in the previous link:2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern[blog post] is real). CDC ability to connect to existing system without any schema nor code changes or even noticeable performance impact is also a big plus.

We chose Debezium over alternative solutions because of its ability to handle MySQL GTID based replication and failovers, ease of writing single message transforms, great documentation and community support from Red Hat over https://gitter.im/debezium/user[gitter chat]. Some other tools had better operational performance, but higher load on the database server as they stored all the internal state (configuration, offsets, schemas) on the source database.

== Architecture

We use debezium to only capture changes done in database and not entire state.

We’ve found that operating Debezium at scale requires quite large management layer. For example even through Debezium supports high-available setups, there’s no built in a way to assign multiple servers to one connector and this must be solved externally.

=== MySQL high available

We have two types of high available MySQL clusters: a large number of *active-passive server* pairs and one big *multi-master cluster*.

++++
<div class="imageblock centered-image">
    <img src="/images/pipedrive-active-passive-db.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Active Passive setup">
</div>
++++

The cluster fallback mechanism differs between the two set-ups:

* Active-passive cluster: virtual IP in front of master handles automatic failover to slave
* Multi-master: MySQL MySQL databases use DNS to just route to another master node

++++
<div class="imageblock centered-image">
    <img src="/images/pipedrive-multi-master-cluster.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Multi-master setup">
</div>
++++

*What exactly happens in case of failover*

Failover starts when

* MySQL server crashes / becomes overloaded / is stopped for maintenance
* Debezium binlog connection is interrupted by “broken pipe” or “EOF” exception.
* Debezium connector is set to FAILED state
* Slave MySQL instance is promoted to new master and starts serving live traffic

To automatically reconnect Debezium we’ve scripted small automation that:

* Detects the Debezium connector is in failed state and restarts the task.
* Debezium tries to connect to MySQL server. Note that from the same IP or URI another MySQL server (with new GTID channel!) will respond.
* Debezium merges existing MySQL offset to what is available on new MySQL server and continues the binlog tailing.

++++
<div class="imageblock centered-image">
    <img src="/images/pipedrive-failover.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Failover">
</div>
++++

Proper failover handling and our so far only code contribution is tracked in https://issues.jboss.org/browse/DBZ-923[DBZ-923] ticket.

=== Debezium clustering - distributed mode, 1 node clusters

Having spent much time trying to stabilize Debezium in kafka-connect distributed mode we gave up and started looking alternatives for jobs distribution. The main problem was one failing job could knock out all others in the same cluster. Rebalance event issue is known, but as of know is still open https://issues.apache.org/jira/browse/KAFKA-5505[#5505]. Wise guys at Red Hat recommended looking towards standalone mode...

We ended up using Debezium in the kafka-connect *distributed mode*, but with *“1 node” size independent clusters*. Instead of having few large nodes sharing 100+ connectors, we have a large number of small Debezium instances. Each instance has unique group.id + status/config/offset/history kafka topic so they don’t form one cluster and don’t share anything between themselves. With 1 node cluster we lose kafka connect distributed mode ability to automatically recover task on other node, but for this we can rely on docker orchestrator.

This may sound a lot like running Debezium in standalone mode and it is very similar, with slight difference. The “1 node” clustered Debezium containers are still stateless in term of storage - they use Kafka broker to store job configurations, offsets and database schema history. We don’t need to attach permanent storage to container and can start the job on any host with the correct groupID. It will pull configuration and offset from central kafka and continue where the last instance left of. Ability to use official Debezium Docker images is also a big plus.

== Management layer

With hundreds of MySQL servers added and removed daily, creating and updating Debezium jobs manually gets out of hand. Automation to the rescue! We created a service called Debezium-manager that  automates the creation of Debezium connector instances. This manager listens to our service discovery; when it sees a new MySQL server, it automatically deploys a Debezium container with correct environment variables pointing to the new MySQL server. Kind of auto scaling.

There’s even a basic *UI for operations* for getting a status overview and easily fixing common problems, like an inconsistent in-memory database schema by running schema recovery to history topic.

++++
<div class="imageblock centered-image">
    <img src="/images/pipedrive-debezium-manager-ui.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Debezium operations UI">
</div>
++++

== Extending Debezium Docker images

Based on Debezium’s official Docker image, we have built a custom one that adds a small Node.js process used for bootstrapping the connector, providing secure access to the Kafka Connect REST API and connector health monitoring.

++++
<div class="imageblock centered-image">
    <img src="/images/pipedrive-dbz-docker.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Debezium extended docker image">
</div>
++++

=== Debezium job creation based on template and env variables
As we use 1 container per 1 job we’ve added Debezium *connector creation functionality into the Debezium container itself*, so we don’t need to make external HTTP requests to start or update connectors. This has become very convenient, as spinning up a container is all it takes to connect to a MySQL server. The Kafka Connect REST API is still exposed through a secured proxy for operational access.

* The Debezium process is started using docker-entrypoint.sh
* A Node.js process is started too, which checks the Kafka Connect REST API port
* When the localhost Kafka Connect REST API port is responsive
* Render template to actual connector configuration
* Check if the connector exists
** If it doesn’t, create it using HTTP POST request
** If it does, but existing configuration doesn’t match our rendered one, update existing using PUT request
** do nothing if connector exists and has latest configuration

We added these new environment variables to Debezium official container.

*JOB_AUTO_CREATE* - on startup renders job template and creates or updates connector using localhost REST API.

*JOB_AUTO_RESTART* - try to restart failed connector on failure

*JOB_TEMPLATE* - which json file to take load as connector configuration template.

+ other env variables that can be referenced from the template.

The job templates are stored inside container image on build time. Templates allowed us to build version of the Debezium image once and reuse it on various MySQL server setups. Templating also has conditionals support to enable some parameters only in specific environment. No more manual curl’ing.

Example connector job template:

[source,json]
----
{
 "tasks.max": "1",
 "connector.class": "io.Debezium.connector.MySQL.MySQLConnector",

 "database.hostname": "{MySQL.host}",
 "database.port": "{MySQL.port}",
 "database.user": "{MySQL.username}",
 "database.password": "{MySQL.password}",
 "database.server.id": "{Debezium.server.id}",
 "database.server.name": "company_db_{baseID}",

 "database.history.kafka.bootstrap.servers": "{kafka.servers}",
 "database.history.kafka.topic": "Debezium-history-company-db{baseID}",
 "database.history.store.only.monitored.tables.ddl": true,
 "database.history.skip.unparseable.ddl": "true",

 "ddl.parser.mode": "antlr",
 "snapshot.mode": "schema_only",

 "snapshot.locking.mode": "none",
 "only.in.env.test.snapshot.locking.mode": "minimal",

 "gtid.new.channel.position": "earliest"
}
----

== Connector health monitoring and automatic restart.

As Debezium connector can’t automatically recover from MySQL connection issues, it needs another management script to restart the task. At first we used external cron like script for this, but now as nodejs is already running in container, we delegated this responsibility there as well.

The logic is very simple - every 30 seconds it connects to the Kafka Connect REST API running in the local container and retrieves the connector status. If the connector or task is in FAILED state, it tries to restart it by posting connectors/jobname/tasks/1/restart up to 5 times. If still doesn’t work or there’s UNASSIGNED state, starts graceful shutdown of the Docker container.

== Securing the Kafka Connect REST API
Because the Kafka Connect REST API had no built in security (in the beginning of 2018 at least), anybody could query and see Kafka cluster and MySQL credentials in plaintext. To prevent such leak we’ve added simple nodejs http proxy script with http basic auth to different port and only expose this port to outside. Once auth is passed this nodejs proxy just forwards all the requests to kafka connect local port 8080.
https://issues.jboss.org/browse/DBZ-702[DBZ-702]

== Monitoring with Prometheus/Grafana
Debezium exposes nice JMX variables, but to query them using prometheus scraper we added prometheus exporter into Debezium docker image. This library has worked without problems https://github.com/prometheus/jmx_exporter[jmx_prometheus_javaagent] and we’re most Debezium and kafka connect prefixed variables as prometheus metrics.

Our main alerts are based on https://debezium.io/docs/connectors/mysql/#binlog-metrics[MySQL_binlog_connected metric], but unfortunately it hasn’t proven absolutely reliable, so there is additional alerts based on logs, counting “org.apache.kafka.connect.errors.ConnectException” exceptions.

Our grafana dashboard looks like this
++++
<div class="imageblock centered-image">
    <img src="/images/pipedrive-grafana.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Debezium extended docker image">
</div>
++++

== Problems and workarounds

=== Databases with super large schemas.
Debezium recommends giving connector 256MB memory for database with schema up to 10000 columns, but some of our servers have ~1.2 million columns! Debezium does work, but requires huge amount of memory to accommodate the in-memory schema. Also some operations that normally take  seconds start to crawl. For example database initial (schema only!) snapshot can run 15+ minutes and fill the history topic with half million “drop table”, “create table” messages. This in turn slows down connector restarts as all those database alters need to be played back to reconstruct correct in memory state.

So what we’ve done so far (to make it perform acceptable level) is apply strict table whitelist for tracking 25% of the most critical tables. For large history topic configured connector to only contain whitelisted table DDLs. Also increasing java heap size to ~2GB has so far proven reliable.

[source,json]
----
database.history.store.only.monitored.tables.ddl: "true",
database.history.skip.unparseable.ddl": "true",
----

=== Snapshots without locking
You can configure connector to use locking.mode=’none’, but it’s dangerous as ALTERS running during the snapshot will not be reflected in the Debezium in-memory database schema. You will learn about it only when the first data change event arrives for that table, which could be weeks after snapshot. So far the fix that worked has been to run schema recovery asap.

== Lessons learned
*Kafka Connect in distributed mode is potentially unstable*. Kafka Connect clustering doesn’t play well with large numbers of connectors and slow task startup time - one connector restart can cause a rebalance event, which means one repeatedly failing MySQL connector can halt whole cluster.

*Manual Debezium offsets adjusting has saved us many times* - storing offsets in external kafka topic allowed us to easily adjust connector’s last offset by producing valid json message and restarting container. With active-passive setup unfortunate server failure can introduce new gaps in gtid ranges or slave or debezium itself can become ahead of new master on particular gtid channel. Fixing this without reseting connector needs manual intervention to last offset.

*Schema / DDL parser problems really hurt*. There’s no easy way out as the alters breaking the Debezium DDL parser can come from any database/table (whitelist doesn't apply) and is also written into history topic. Even if you push the Debezium offset manually over the failing event you’ll crash again while reading from the history topic. Corrupted history topic will break connector sooner or later. The only workaround that seems to work is running a schema recovery, but it's only possible when no other alters have been executed during downtime. Fortunately the DDL parser issues reported got fixed rapidly and debezium developers helped to narrow down the exact problem from the logs. Related issues:
https://issues.jboss.org/browse/DBZ-476[DBZ-476]
https://issues.jboss.org/browse/DBZ-864[DBZ-864]
https://issues.jboss.org/browse/DBZ-872[DBZ-872]
https://issues.jboss.org/browse/DBZ-876[DBZ-876]
https://issues.jboss.org/browse/DBZ-863[DBZ-863]
https://issues.jboss.org/browse/DBZ-901[DBZ-901]
https://issues.jboss.org/browse/DBZ-903[DBZ-903]

And a list of other operational issues we've seen over time:
https://issues.jboss.org/browse/DBZ-682[DBZ-682]
https://issues.jboss.org/browse/DBZ-693[DBZ-693]
https://issues.jboss.org/browse/DBZ-859[DBZ-859],
https://issues.jboss.org/browse/DBZ-1201[DBZ-1201]
https://issues.jboss.org/browse/DBZ-1202[DBZ-1202]

This may seem like a lot, but mostly they are corner cases.

Once you get it running stable you’ll see uptime for months.

== Future

Change data capture remains a huge part of our system and has proven to be consistent and reliable. Setting up and operations hasn’t been the smoothest, but it does work and is delivering the initial goal of zero data loss. We have production services depending on it every day and more being built.

Our next steps involve utilizing events more. There is a growing need for real time data processing and having reliable event stream is vital for it.

We plan to continue working with Debezium developers to improve the open source CDC software and make running it large smoother experience. Thanks Gunnar Morling and Jiri Pechanec for driving this project and taking time to discuss and solve our challenges.

== About Pipedrive

https://www.pipedrive.com[Pipedrive] is a sales management tool designed to help small sales teams manage intricate or lengthy sales processes. Grounded in the philosophy of activity-based selling, the result is Pipedrive, a tool now used by over 80,000 companies around the world. Pipedrive has raised $90 million from Atomico, Bessemer Venture Partners, Rembrandt Venture Partners, Paua Ventures and others, and are proud to be part of AngelPad alumni.

== About Debezium

Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of http://kafka.apache.org/[Kafka] and provides http://kafka.apache.org/documentation.html#connect[Kafka Connect] compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is link:/license/[open source] under the http://www.apache.org/licenses/LICENSE-2.0.html[Apache License, Version 2.0].

== Get involved

We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter https://twitter.com/debezium[@debezium], https://gitter.im/debezium/user[chat with us on Gitter],
or join our https://groups.google.com/forum/#!forum/debezium[mailing list] to talk with the community.
All of the code is open source https://github.com/debezium/[on GitHub],
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or https://issues.jboss.org/projects/DBZ/issues/[log an issue].
