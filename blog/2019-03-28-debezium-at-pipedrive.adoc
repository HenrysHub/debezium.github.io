= Debezium at Pipedrive
ekoplimets
:awestruct-tags: [ pipedrive, examples, microservices, apache-kafka ]
:awestruct-layout: blog-post

[role="teaser"]
--
Intro here
--

Pipedrive started looking into change data capture solutions for MySQL in the beginning of 2018 and now have most of the production MySQL databases producing real-time CDC events using Debezium. The following blog post describes our setup, various problems we have encountered and how we resolved them.

To start of with some numbers to get an idea of scale - we have in production environment *250+ MySQL active-passive server pairs* and one huge *10 node MySQL multi-master xtraDB cluster* spanning multiple data centers. Average incoming messages rate to kafka clusters is around 1500 msg/sec with peaks up to 10k msg/sec.

Our initial goals were to have real-time and 100% consistent data source for services depending on our main data entities, like full text search, statistics database and data warehouse. It has also helped some of our services to remove a hard dependency on message queues availability (the dual write problem mentioned in the previous link:2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern[blog post] is real). CDC ability to connect to existing system without any schema nor code changes or even noticeable performance impact is also a big plus.

We chose Debezium over alternative solutions because of its ability to handle MySQL GTID based replication and failovers, ease of writing single message transforms, great documentation and community support from Red Hat over https://gitter.im/debezium/user[gitter chat]. Some other tools had better operational performance, but higher load on the database server as they stored all the internal state (configuration, offsets, schemas) on the source database.

== Architecture

We use debezium to only capture changes done in database and not entire state.

We’ve found that operating Debezium at scale requires quite large management layer. For example even through Debezium supports high-available setups, there’s no built in a way to assign multiple servers to one connector and this must be solved externally.

=== MySQL high available

We have two types of high available MySQL clusters: a large number of *active-passive server* pairs and one big *multi-master cluster*.

++++
<div class="imageblock centered-image">
    <img src="/images/pipedrive-active-passive-db.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Active Passive setup">
</div>
++++

The cluster fallback mechanism differ between the two set-ups:

* Active-passive cluster: virtual IP in front of master handles automatic failover to slave
* Multi-master: MySQL MySQL databases use DNS to just route to another master node

++++
<div class="imageblock centered-image">
    <img src="/images/pipedrive-multi-master-cluster.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Multi-master setup">
</div>
++++

*What exactly happens in case of failover*

Failover starts when

* MySQL server crashes / becomes overloaded / is stopped for maintenance
* Debezium binlog connection is interrupted by “broken pipe” or “EOF” exception.
* Debezium connector is set to FAILED state
* Slave MySQL instance is promoted to new master and starts serving live traffic

To automatically reconnect Debezium we’ve scripted small automation that:

* Detects Debezium connector status is in failed state and restarts the task.
* Debezium tries to connect to MySQL server. Note that from the same ip or uri another MySQL server (with new gtid channel!) will respond.
* Debezium merges existing MySQL offset to what is available on new MySQL server and continues the binlog tailing.

++++
<div class="imageblock centered-image">
    <img src="/images/pipedrive-failover.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Failover">
</div>
++++

=== Debezium clustering - distributed mode, 1 node clusters

Having spent much time trying to stabilize Debezium in kafka-connect distributed mode we gave up and started looking alternatives for jobs distribution. The main problem was one failing job could knock out all others in the same cluster. Rebalance event issue is known, but as of know is still open https://issues.apache.org/jira/browse/KAFKA-5505[#5505]. Wise guys at Red Hat recommended looking towards standalone mode...

We ended up using Debezium in the kafka-connect *distributed mode*, but with *“1 node” size independent clusters*. Instead of having few large nodes sharing 100+ connectors, we have a large number of small Debezium instances. Each instance has unique group.id + status/config/offset/history kafka topic so they don’t form one cluster and don’t share anything between themselves. With 1 node cluster we lose kafka connect distributed mode ability to automatically recover task on other node, but for this we can rely on docker orchestrator.

This may sound a lot like running Debezium in standalone mode and it is very similar, with slight difference. The “1 node” clustered Debezium containers are still stateless in term of storage - they use Kafka broker to store job configurations, offsets and database schema history. We don’t need to attach permanent storage to container and can start the job on any host with the correct groupID. It will pull configuration and offset from central kafka and continue where the last instance left of. Ability to use official Debezium Docker images is also a big plus.

== Management layer

With hundreds of MySQL servers added and removed daily, creating and updating Debezium jobs manually gets out of hand. Automation to the rescue! We created a service called Debezium-manager that  automates the creation of Debezium connector instances. This manager listens our service-discovery and when sees new MySQL server automatically deploys Debezium container with correct environment variables pointing to the new MySQL server. Kind of auto scaling.

There’s even basic *UI for operations* to get status overview and ease fixing common problems, like fixing corrupted in-memory database schema by running schema recovery to history topic.

++++
<div class="imageblock centered-image">
    <img src="/images/pipedrive-debezium-manager-ui.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Debezium operations UI">
</div>
++++

== Extending Debezium Docker images

Based on Debezium’s official Docker image, we have built a custom one that adds a small Node.js process used for bootstrapping the connector, providing secure access to the Kafka Connect REST API and connector health monitoring.

++++
<div class="imageblock centered-image">
    <img src="/images/pipedrive-dbz-docker.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Debezium extended docker image">
</div>
++++

=== Debezium job creation based on template + env variables.
As we use 1 container per 1 job we’ve added Debezium *connector creation functionality into Debezium container itself*, so we don’t need to make external HTTP request to start or update connectors. This has become very convenient as spinning up container is all it takes to connect to mysql server. The Kafka Connect REST API is still exposed through a secured proxy for operational access.

* Debezium process is started using docker-entrypoint.sh
* another nodejs process is started also, which checks Kafka Connect REST API port.
* When the localhost Kafka Connect REST API port is responsive
* Render template to actual connector configuration
* Check if the connector exists
** If it doesn’t, create it using HTTP POST request
** If it does, but existing configuration doesn’t match our rendered one, update existing using PUT request
** do nothing if connector exists and has latest configuration.

We added these new environment variables to Debezium official container.

*JOB_AUTO_CREATE* - on startup renders job template and creates or updates connector using localhost REST API.

*JOB_AUTO_RESTART* - try to restart failed connector on failure

*JOB_TEMPLATE* - which json file to take load as connector configuration template.

+ other env variables that can be referenced from the template.

The job templates are stored inside container image on build time. Templates allowed us to build version of the Debezium image once and reuse it on various MySQL server setups. Templating also has conditionals support to enable some parameters only in specific environment. No more manual curl’ing.

Example connector job template:

[source,json]
----
{
 "tasks.max": "1",
 "connector.class": "io.Debezium.connector.MySQL.MySQLConnector",

 "database.hostname": "{MySQL.host}",
 "database.port": "{MySQL.port}",
 "database.user": "{MySQL.username}",
 "database.password": "{MySQL.password}",
 "database.server.id": "{Debezium.server.id}",
 "database.server.name": "company_db_{baseID}",

 "database.history.kafka.bootstrap.servers": "{kafka.servers}",
 "database.history.kafka.topic": "Debezium-history-company-db{baseID}",
 "database.history.store.only.monitored.tables.ddl": true,
 "database.history.skip.unparseable.ddl": "true",

 "ddl.parser.mode": "antlr",
 "snapshot.mode": "schema_only",

 "snapshot.locking.mode": "none",
 "only.in.env.test.snapshot.locking.mode": "minimal",

 "gtid.new.channel.position": "earliest"
}
----

== Connector health monitoring and automatic restart.

As Debezium connector can’t automatically recover from MySQL connection issues, it needs another management script to restart the task. At first we used external cron like script for this, but now as nodejs is already running in container, we delegated this responsibility there as well.

The logic is very simple - every 30 seconds it connects to the Kafka Connect REST API running in the local container and retrieves the connector status. If the connector or task is in FAILED state, it tries to restart it by posting connectors/jobname/tasks/1/restart up to 5 times. If still doesn’t work or there’s UNASSIGNED state, starts graceful shutdown of the Docker container.

== Securing the Kafka Connect REST API
Because the Kafka Connect REST API had no built in security (in the beginning of 2018 at least), anybody could query and see Kafka cluster and MySQL credentials in plaintext. To prevent such leak we’ve added simple nodejs http proxy script with http basic auth to different port and only expose this port to outside. Once auth is passed this nodejs proxy just forwards all the requests to kafka connect local port 8080.

== Monitoring with Prometheus/Grafana
Debezium exposes nice JMX variables, but to query them using prometheus scraper we added prometheus exporter into Debezium docker image. This library has worked without problems https://github.com/prometheus/jmx_exporter[jmx_prometheus_javaagent] and we’re most Debezium and kafka connect prefixed variables as prometheus metrics.

Our main alerts are based on https://debezium.io/docs/connectors/mysql/#binlog-metrics[MySQL_binlog_connected metric], but unfortunately it hasn’t proven absolutely reliable, so there is additional alerts based on logs, counting “org.apache.kafka.connect.errors.ConnectException” exceptions.

Our grafana dashboard looks like this
++++
<div class="imageblock centered-image">
    <img src="/images/pipedrive-grafana.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Debezium extended docker image">
</div>
++++

== Problems and workarounds

=== Databases with super large schemas.
Debezium recommends giving connector 256MB memory for database with schema up to 10000 columns, but some of our servers have ~1.2 million columns! Debezium does work, but requires huge amount of memory to accommodate the in-memory schema. Also some operations that normally take  seconds start to crawl. For example database initial (schema only!) snapshot can run 15+ minutes and fill the history topic with half million “drop table”, “create table” messages. This in turn slows down connector restarts as all those database alters need to be played back to reconstruct correct in memory state.

So what we’ve done so far (to make it perform acceptable level) is apply strict table whitelist for tracking 25% of the most critical tables. For large history topic configured connector to only contain whitelisted table DDLs. Also increasing java heap size to ~2GB has so far proven reliable.

[source,json]
----
database.history.store.only.monitored.tables.ddl: "true",
database.history.skip.unparseable.ddl": "true",
----

=== Snapshots without locking
You can configure connector to use locking.mode=’none’, but it’s dangerous as ALTERS running during the snapshot will not be reflected in the Debezium in-memory database schema. You will learn about it only when the first data change event arrives for that table, which could be weeks after snapshot. So far the fix that worked has been to run schema recovery asap.

== Lessons learned
*Kafka connect in distributed mode is unstable*. Kafka-connect cluster doesn’t play well with large connectors count and slow task startup time - one connector restart can cause rebalance event, which means one repeatedly failing MySQL connector can halt whole cluster.

*Manual Debezium offsets adjusting has saved us many times* - storing offsets in external kafka topic allowed us to easily adjust connector’s last offset by producing valid json message and restarting container. With active-passive setup unfortunate server failure can introduce new gaps in gtid ranges or slave or debezium itself can become ahead of new master on particular gtid channel. Fixing this without reseting connector needs manual intervention to last offset.

*Schema / ddl parser problems really hurt*. There’s no easy way out as the alter breaking Debezium ddl parser can come from any database/table (whitelist doesn't apply) and is also written into history topic. Even if you push the debezium offset manually over the failing event you’ll crash again while reading from the history topic. Only thing that works is running schema recovery, but this only when no other alters have been executed.

But once you get it running stable you’ll see uptime for months.

== Future

Change data capture remains a huge part of our system and has proven to produce consistent and reliable data. Setting up and operations hasn’t been the smoothest, but it does work delivering initial goal of zero data loss. We have production services depending on it every day and more being built.

There is also a growing need for real time data processing and having reliable event stream becomes vital.

Some of our microservices moving to event sourcing patterns, in which database becomes convenient materialized view and the truth is stored in events.

== About Pipedrive

...

== About Debezium

Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of http://kafka.apache.org/[Kafka] and provides http://kafka.apache.org/documentation.html#connect[Kafka Connect] compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is link:/license/[open source] under the http://www.apache.org/licenses/LICENSE-2.0.html[Apache License, Version 2.0].

== Get involved

We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter https://twitter.com/debezium[@debezium], https://gitter.im/debezium/user[chat with us on Gitter],
or join our https://groups.google.com/forum/#!forum/debezium[mailing list] to talk with the community.
All of the code is open source https://github.com/debezium/[on GitHub],
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or https://issues.jboss.org/projects/DBZ/issues/[log an issue].
